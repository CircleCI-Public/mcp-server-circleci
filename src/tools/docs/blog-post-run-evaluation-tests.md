# Run comprehensive prompt evaluations directly from your IDE

_Engineering Productivity • [Date] • 6 min read_

**Ryan Hamilton**  
_Senior Software Engineer_

---

You've structured your prompts into templates. You've generated comprehensive test suites. Now comes the moment of truth: does your AI feature actually work as intended across all those scenarios?

Running prompt evaluations used to mean switching between your IDE, testing frameworks, CI dashboards, and results analysis tools. You'd trigger tests, wait for results, dig through logs, and piece together performance insights across multiple interfaces. It breaks focus and slows down iteration cycles.

This post shows how to execute complete prompt evaluation pipelines directly from your IDE using the `run_evaluation_tests` tool in the CircleCI MCP Server. You'll type a simple command like:

```plaintext
Run evaluation tests for all my prompt templates
```

The assistant triggers a full evaluation pipeline in CircleCI, then returns detailed results showing which test cases pass, fail, or need attention—all without leaving your editor.

## Why this is useful

Having comprehensive test suites is only valuable if you can run them easily and frequently. This tool removes friction at crucial moments:

- You've updated a prompt template and want to verify nothing broke
- You're comparing multiple template versions to choose the best one
- You're preparing for production deployment and need confidence in your AI features
- You're debugging specific failures and need to isolate which scenarios work
- You're collaborating with teammates and need shared, reproducible evaluation standards
- You're iterating rapidly and want instant feedback on prompt changes

In all these scenarios, the ability to trigger comprehensive evaluations and get detailed results immediately accelerates your development cycle. Instead of context switching between tools, everything happens in your coding environment.

It completes the workflow started by `create_prompt_template` and `recommend_prompt_template_tests`, giving you end-to-end prompt development without leaving your IDE.

## Prerequisites

Before you begin, make sure you have:

- A CircleCI account with projects set up
- Node.js 18 or later
- An IDE that supports the CircleCI MCP Server (like Cursor)
- At least one prompt template in your `./prompts` directory
- Test cases generated using the `recommend_prompt_template_tests` tool

You'll also need CircleCI configured to access your repository and run evaluation pipelines.

## Preparing your environment

To run evaluation tests, you'll need the CircleCI MCP Server connected to both your IDE and your CircleCI account. This gives the assistant permission to trigger evaluation pipelines and retrieve results.

The tool automatically generates appropriate CircleCI configuration for prompt evaluation, handles test execution across your template suite, and surfaces results in a developer-friendly format.

## Step 1: Prepare your prompt templates

Make sure you have structured prompt templates in your `./prompts` directory. Each template should follow the standard format with clear parameter definitions and context schemas.

If you don't have templates yet, use the `create_prompt_template` tool:

```plaintext
Create prompt templates for all the AI features in my application
```

Once you have templates, ensure they have associated test cases generated by the `recommend_prompt_template_tests` tool. The evaluation pipeline runs these test cases against your templates to measure performance.

## Step 2: Trigger evaluation tests

Open the AI assistant in your IDE and request evaluation for your templates:

```plaintext
Run evaluation tests for all my prompt templates
```

The assistant uses the `run_evaluation_tests` tool to:

1. **Generate a CircleCI configuration** - Creates pipeline YAML optimized for prompt evaluation
2. **Trigger the evaluation pipeline** - Starts the test run in CircleCI with your template files
3. **Return a monitoring URL** - Gives you a link to track progress and view detailed results

You can also run evaluations for specific templates:

```plaintext
Run evaluation tests for my customer-support-chatbot.prompt.yml template only
```

Or test specific scenarios:

```plaintext
Run evaluation tests for edge cases in my bedtime story template
```

## Step 3: Monitor evaluation progress

The tool returns a CircleCI pipeline URL where you can monitor test execution in real-time. The evaluation pipeline runs through your entire test suite, executing each test case against your prompt template and collecting results.

While tests are running, you can continue coding. The evaluation happens in the background, and you'll get comprehensive results once complete.

## Step 4: Review detailed results

Once evaluation completes, ask the assistant for results:

```plaintext
What were the results of my latest evaluation tests?
```

The assistant retrieves and analyzes the test results, providing:

**Overall performance summary:**

- How many test cases passed, failed, or had issues
- Performance trends compared to previous runs
- Key insights about template reliability

**Detailed test case results:**

- Which specific scenarios succeeded or failed
- Output quality assessment for each test case
- Failure analysis with specific issues identified

**Actionable recommendations:**

- Specific prompt improvements based on failures
- Edge cases that need attention
- Template modifications to improve performance

Here's what typical results might look like:

```
Evaluation Results Summary:
✅ 23 test cases passed (77%)
❌ 5 test cases failed (17%)
⚠️  2 test cases had warnings (6%)

Key Findings:
- Edge cases with very young ages (2-3 years) need simpler language
- Complex topics require better age-appropriate explanations
- Story endings are consistently calm and appropriate

Recommended Actions:
1. Adjust language complexity detection for ages under 4
2. Add topic complexity assessment logic
3. Consider expanding test coverage for cultural sensitivity
```

## Step 5: Iterate based on results

With detailed evaluation results, you can immediately improve your templates:

```plaintext
Based on these results, please update my bedtime story template to handle young children better
```

The assistant can modify your template to address specific issues found during evaluation. Then you can re-run tests to verify improvements:

```plaintext
Run evaluation tests again to verify my template changes
```

This creates a tight feedback loop: evaluate, analyze, improve, re-evaluate.

## Step 6: Scale across your AI features

The real power emerges when you use this systematic evaluation across all your AI development:

**Compare template versions:**

```plaintext
Run evaluation tests for both versions of my customer support template and compare results
```

**Monitor template performance over time:**

```plaintext
Run daily evaluation tests and alert me if performance drops
```

**Validate before deployment:**

```plaintext
Run comprehensive evaluation tests before I merge this PR
```

This transforms prompt development from manual testing to automated quality assurance that scales with your AI features.

## Integrating with your development workflow

The evaluation pipeline integrates naturally with standard development practices:

- **Pre-commit testing** - Validate prompt changes before committing
- **CI/CD integration** - Automatic evaluation on pull requests
- **Performance monitoring** - Regular evaluation to catch regressions
- **A/B testing** - Compare different prompt versions objectively

Instead of separate AI tooling, prompt evaluation becomes part of your normal development flow.

## Conclusion

The jump from "I think this prompt works" to "I know this prompt works reliably" is what separates experimental AI features from production-ready ones. When you can run comprehensive evaluations as easily as running unit tests, that confidence comes naturally.

The `run_evaluation_tests` tool doesn't just execute test cases—it gives you systematic insight into prompt reliability across all the scenarios that matter. Instead of hoping your AI features work in production, you validate them thoroughly during development.

Combined with structured templates and generated test suites, you get a complete prompt development workflow that rivals traditional software testing practices. That's the foundation for building AI features your team and users can depend on.

You can get started by running evaluation tests on any prompt templates you've created. Once you experience the confidence that comes from systematic evaluation, manual testing will feel like guesswork.

---

_Engineering Productivity_

## Similar posts you may enjoy

- **Turn your AI ideas into testable prompt templates without leaving your IDE** - [Date] - 6 min read
- **Systematically test your AI prompts with automated evaluation suites** - [Date] - 5 min read
- **Check the status of your CircleCI pipeline without leaving your IDE** - May 30, 2025 - 5 min read
